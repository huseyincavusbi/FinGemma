{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth","metadata":{"id":"y3zJW6IlZhL9","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:30:43.160946Z","iopub.execute_input":"2025-08-15T09:30:43.161170Z","iopub.status.idle":"2025-08-15T09:30:56.777514Z","shell.execute_reply.started":"2025-08-15T09:30:43.161147Z","shell.execute_reply":"2025-08-15T09:30:56.776786Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nCollecting xformers==0.0.29.post3\n  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\nCollecting trl\n  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\nCollecting cut_cross_entropy\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting unsloth_zoo\n  Downloading unsloth_zoo-2025.8.4-py3-none-any.whl.metadata (9.4 kB)\nDownloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.21.0-py3-none-any.whl (511 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading unsloth_zoo-2025.8.4-py3-none-any.whl (181 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.9/181.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: xformers, unsloth_zoo, trl, cut_cross_entropy, bitsandbytes\nSuccessfully installed bitsandbytes-0.47.0 cut_cross_entropy-25.1.1 trl-0.21.0 unsloth_zoo-2025.8.4 xformers-0.0.29.post3\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (3.20.3)\nRequirement already satisfied: datasets<4.0.0,>=3.4.1 in /usr/local/lib/python3.11/dist-packages (3.6.0)\nCollecting huggingface_hub>=0.34.0\n  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (0.1.9)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (2.32.4)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0) (1.1.5)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (3.12.13)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets<4.0.0,>=3.4.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets<4.0.0,>=3.4.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets<4.0.0,>=3.4.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets<4.0.0,>=3.4.1) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets<4.0.0,>=3.4.1) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets<4.0.0,>=3.4.1) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets<4.0.0,>=3.4.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets<4.0.0,>=3.4.1) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets<4.0.0,>=3.4.1) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets<4.0.0,>=3.4.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets<4.0.0,>=3.4.1) (2024.2.0)\nDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, huggingface_hub\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.33.1\n    Uninstalling huggingface-hub-0.33.1:\n      Successfully uninstalled huggingface-hub-0.33.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nunsloth-zoo 2025.8.4 requires msgspec, which is not installed.\nunsloth-zoo 2025.8.4 requires tyro, which is not installed.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 huggingface_hub-0.34.4\nCollecting unsloth\n  Downloading unsloth-2025.8.5-py3-none-any.whl.metadata (47 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth-2025.8.5-py3-none-any.whl (307 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.6/307.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: unsloth\nSuccessfully installed unsloth-2025.8.5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom unsloth import FastLanguageModel\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\nfrom datasets import load_dataset\n\n# 1. Load the Model\nmax_seq_length = 2048\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/gemma-3-270m-it\",\n    max_seq_length = max_seq_length,\n    load_in_4bit = True,\n    dtype = None, \n)\n\n# 2. Configure LoRA Adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Rank of the adapters\n    lora_alpha = 16, # Scaling factor\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = True,\n    random_state = 42,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n)\n\nprint(\"Unsloth model configured for 4-bit LoRA fine-tuning!\")","metadata":{"id":"FrYdrNcHaLAb","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:31:42.084559Z","iopub.execute_input":"2025-08-15T09:31:42.084858Z","iopub.status.idle":"2025-08-15T09:32:41.469740Z","shell.execute_reply.started":"2025-08-15T09:31:42.084825Z","shell.execute_reply":"2025-08-15T09:32:41.468889Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-08-15 09:31:54.371476: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755250314.736072      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755250314.809402      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.8.5: Fast Gemma3 patching. Transformers: 4.52.4.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Using float16 precision for gemma3 won't work! Using float32.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92abc201d6114078b53881fd2eaeba89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e75b497f5ea44feaa6e9dac539984f15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5e5b75e5f934e4693ce23e5cb47facf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe223c55199b4086b05cf5b2495763a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"770ee9c0bfd0486ba2745f620a4f4405"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3179d32ea1634d85bd1d0139277d233e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c6d6ec58934d839823bf761a154c2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b44e917602894f1e8f514683ddf30a49"}},"metadata":{}},{"name":"stdout","text":"Unsloth: Making `model.base_model.model.model` require gradients\nUnsloth model configured for 4-bit LoRA fine-tuning!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma3\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:32:46.119905Z","iopub.execute_input":"2025-08-15T09:32:46.120180Z","iopub.status.idle":"2025-08-15T09:32:46.125249Z","shell.execute_reply.started":"2025-08-15T09:32:46.120160Z","shell.execute_reply":"2025-08-15T09:32:46.124413Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load Datasets and Merge them, then take a 60k random sample\nfrom datasets import load_dataset, concatenate_datasets\n\ndef load_merge_and_sample_finance_datasets(num_samples=60000):\n    \"\"\"\n    Loads two finance datasets, merges them, and returns a random sample\n    of the specified size.\n    \"\"\"\n    print(\"Loading gbharti/wealth-alpaca_lora dataset...\")\n    wealth_ds = load_dataset(\"gbharti/wealth-alpaca_lora\", split=\"train\")\n\n    print(\"Loading Josephgflowers/Finance-Instruct-500k dataset...\")\n    finance_ds = load_dataset(\"Josephgflowers/Finance-Instruct-500k\", split=\"train\")\n\n    # --- Preprocessing functions ---\n    def preprocess_wealth_alpaca(example):\n        if example.get('input'):\n            example['instruction'] = f\"{example['instruction']}\\n{example['input']}\"\n        return {\"instruction\": example[\"instruction\"], \"output\": example[\"output\"]}\n\n    def preprocess_finance_instruct(example):\n        return {\"instruction\": example[\"user\"], \"output\": example[\"assistant\"]}\n\n    print(\"Preprocessing datasets...\")\n    wealth_ds = wealth_ds.map(preprocess_wealth_alpaca, remove_columns=wealth_ds.column_names)\n    finance_ds = finance_ds.map(preprocess_finance_instruct, remove_columns=finance_ds.column_names)\n\n    # --- Merging ---\n    print(\"Merging the datasets...\")\n    merged_dataset = concatenate_datasets([wealth_ds, finance_ds])\n    print(f\"Total size of merged dataset: {len(merged_dataset)} rows\")\n\n    # --- Shuffling and Sampling ---\n    print(\"Shuffling the merged dataset to ensure a random sample...\")\n    # Using a seed ensures that you get the same \"random\" sample every time you run the code.\n    shuffled_dataset = merged_dataset.shuffle(seed=42)\n\n    print(f\"Selecting a random sample of {num_samples} rows...\")\n    # The .select() method is highly efficient for taking a slice of the dataset.\n    sampled_dataset = shuffled_dataset.select(range(num_samples))\n    \n    return sampled_dataset\n\n# --- Main Execution ---\n# Call the function to get your final 60k row dataset\ndataset = load_merge_and_sample_finance_datasets(num_samples=60000)\n\nprint(\"Successfully created the 'dataset' variable!\")\nprint(f\"Final dataset size: {len(dataset)} random rows.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:32:48.596703Z","iopub.execute_input":"2025-08-15T09:32:48.596981Z","iopub.status.idle":"2025-08-15T09:33:24.761125Z","shell.execute_reply.started":"2025-08-15T09:32:48.596958Z","shell.execute_reply":"2025-08-15T09:33:24.760335Z"}},"outputs":[{"name":"stdout","text":"Loading gbharti/wealth-alpaca_lora dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/372 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65cba6cc17f141edac06fa9e3b4109ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"final_dataset_clean.json:   0%|          | 0.00/31.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d05e1c8dc2a54587b13ae6ffb7a8e20a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/44341 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8e4b4c4ee4f4ede9e9958b17cb63ee3"}},"metadata":{}},{"name":"stdout","text":"Loading Josephgflowers/Finance-Instruct-500k dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5223027b6c254532b3ad7d95cc0be275"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.json:   0%|          | 0.00/580M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5535884cb01a4fb39fe60fffdba6a11d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/518185 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fff23c17d8234b0f911acc73c7090a01"}},"metadata":{}},{"name":"stdout","text":"Preprocessing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/44341 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9440d95fcbb449d8b5e5bc4cbc1f1031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/518185 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a89de8093f74089b72df97172644622"}},"metadata":{}},{"name":"stdout","text":"Merging the datasets...\nTotal size of merged dataset: 562526 rows\nShuffling the merged dataset to ensure a random sample...\nSelecting a random sample of 60000 rows...\nSuccessfully created the 'dataset' variable!\nFinal dataset size: 60000 random rows.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def convert_to_chatml(example):\n    return {\n        \"conversations\": [\n            {\"role\": \"user\", \"content\": example[\"instruction\"]},\n            {\"role\": \"assistant\", \"content\": example[\"output\"]}\n        ]\n    }\n\ndataset = dataset.map(\n    convert_to_chatml\n)","metadata":{"id":"HFlZlRSofHSJ","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:34:04.830144Z","iopub.execute_input":"2025-08-15T09:34:04.830440Z","iopub.status.idle":"2025-08-15T09:34:10.507857Z","shell.execute_reply.started":"2025-08-15T09:34:04.830419Z","shell.execute_reply":"2025-08-15T09:34:10.507301Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc741977cac04cdbb1dce870eaffd2ea"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"dataset[100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:34:12.844564Z","iopub.execute_input":"2025-08-15T09:34:12.844862Z","iopub.status.idle":"2025-08-15T09:34:12.851784Z","shell.execute_reply.started":"2025-08-15T09:34:12.844840Z","shell.execute_reply":"2025-08-15T09:34:12.851118Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'instruction': 'Where was the May incident recorded?',\n 'output': 'Okfuskee County',\n 'conversations': [{'content': 'Where was the May incident recorded?',\n   'role': 'user'},\n  {'content': 'Okfuskee County', 'role': 'assistant'}]}"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def formatting_prompts_func(examples):\n   convos = examples[\"conversations\"]\n   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n   return { \"text\" : texts, }\n\ndataset = dataset.map(formatting_prompts_func, batched = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:34:14.453231Z","iopub.execute_input":"2025-08-15T09:34:14.453577Z","iopub.status.idle":"2025-08-15T09:34:19.827731Z","shell.execute_reply.started":"2025-08-15T09:34:14.453555Z","shell.execute_reply":"2025-08-15T09:34:19.827060Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80abf9e06dea4368bd742b66fc130e56"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"dataset[100]['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:34:20.873596Z","iopub.execute_input":"2025-08-15T09:34:20.873890Z","iopub.status.idle":"2025-08-15T09:34:20.878959Z","shell.execute_reply.started":"2025-08-15T09:34:20.873860Z","shell.execute_reply":"2025-08-15T09:34:20.878350Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'<start_of_turn>user\\nWhere was the May incident recorded?<end_of_turn>\\n<start_of_turn>model\\nOkfuskee County<end_of_turn>\\n'"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Configure LoRA and Start Training\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size = 64,     \n    gradient_accumulation_steps = 8,      \n    warmup_steps = 10, \n    #max_steps = 100,\n    num_train_epochs = 1,                 \n    learning_rate = 1e-4,                 \n    fp16 = not torch.cuda.is_bf16_supported(),\n    bf16 = torch.cuda.is_bf16_supported(),\n    logging_steps = 25,                   \n    optim = \"adamw_8bit\",                 \n    weight_decay = 0.01,                 \n    lr_scheduler_type = \"linear\",         \n    seed = 42,\n    output_dir = \"outputs\",             \n    report_to = \"none\",                   \n)","metadata":{"id":"vAeqKPaB3N5L","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:36:01.466170Z","iopub.execute_input":"2025-08-15T09:36:01.467000Z","iopub.status.idle":"2025-08-15T09:36:01.500869Z","shell.execute_reply.started":"2025-08-15T09:36:01.466971Z","shell.execute_reply":"2025-08-15T09:36:01.500123Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# --- Initialize Trainer ---\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\", # Point trainer to our formatted 'text' column\n    max_seq_length = max_seq_length,\n    args = training_args,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:36:03.415247Z","iopub.execute_input":"2025-08-15T09:36:03.415893Z","iopub.status.idle":"2025-08-15T09:36:20.414795Z","shell.execute_reply.started":"2025-08-15T09:36:03.415854Z","shell.execute_reply":"2025-08-15T09:36:20.414276Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Switching to float32 training since model cannot work with float16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"]:   0%|          | 0/60000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a0eb5e48a3474c82988c3429f7daad"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# --- Start Fine-tuning ---\nprint(\"Starting the fine-tuning process...\")\ntrainer_stats = trainer.train()\nprint(\"Fine-tuning complete!\")","metadata":{"trusted":true,"id":"r4mYyHi46sTd","execution":{"iopub.status.busy":"2025-08-15T09:36:51.234723Z","iopub.execute_input":"2025-08-15T09:36:51.235444Z","iopub.status.idle":"2025-08-15T12:36:22.491846Z","shell.execute_reply.started":"2025-08-15T09:36:51.235420Z","shell.execute_reply":"2025-08-15T12:36:22.491229Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 60,000 | Num Epochs = 1 | Total steps = 118\nO^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 8\n\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 8 x 1) = 512\n \"-____-\"     Trainable parameters = 3,796,992 of 271,895,168 (1.40% trained)\n","output_type":"stream"},{"name":"stdout","text":"Starting the fine-tuning process...\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='118' max='118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [118/118 2:56:57, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>3.692200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>3.681100</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>3.675300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>3.633900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Fine-tuning complete!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Inference and Saving the Model\nprint(\"\\n--- Running Inference ---\")\nfrom transformers import pipeline\n\n# Use Unsloth's fast inference pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n# Create a test prompt\nmessages = [\n    {\"role\": \"user\", \"content\": \"What are the main risks associated with investing in emerging markets?\"},\n]\n\n# Get the response\noutputs = pipe(messages, max_new_tokens=256, do_sample=True, temperature=0.7, top_p=0.95)\nprint(outputs[0]['generated_text'])","metadata":{"trusted":true,"id":"yBT7xhprF1ZE","execution":{"iopub.status.busy":"2025-08-15T12:40:22.004382Z","iopub.execute_input":"2025-08-15T12:40:22.005641Z","iopub.status.idle":"2025-08-15T12:41:02.701278Z","shell.execute_reply.started":"2025-08-15T12:40:22.005616Z","shell.execute_reply":"2025-08-15T12:41:02.700482Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nThe model 'PeftModel' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n","output_type":"stream"},{"name":"stdout","text":"\n--- Running Inference ---\n[{'role': 'user', 'content': 'What are the main risks associated with investing in emerging markets?'}, {'role': 'assistant', 'content': \"Investing in emerging markets, while potentially offering potential for growth and diversification, comes with a set of significant risks. Here's a breakdown of these risks:\\n\\n*   **Political Instability and Corruption:**\\n    *   **Outdated Monetary Policy:** Emerging market currencies often rely on fiscal policies, which can lead to volatility in central bank policy. This can result in currency depreciation and, in extreme cases, a severe economic downturn, potentially impacting investment returns.\\n    *   **Corruption:** Corruption in emerging markets, particularly in certain sectors (e.g., media, banking, and even certain government services), can lead to significant revenue loss, reduced investment flows, and increased illicit activities.\\n    *   **Uncautious Government:** Ineffective or poorly managed government policies can lead to a lack of transparency, mis-communication, and a shift in priorities. This can result in increased risk for investors.\\n    *   **Central Bank's Weakness:** Central banks in emerging markets may be less responsive to global economic conditions, or lack the capacity to manage their own monetary policy effectively. This can result in lower exchange rates, higher inflation, and reduced investment.\\n*   **Economic Volatility and Market Speculation:**\\n    *   **Commodity\"}]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"id":"Z2EhVdrGF1ZE","execution":{"iopub.status.busy":"2025-08-15T12:43:15.647489Z","iopub.execute_input":"2025-08-15T12:43:15.647728Z","iopub.status.idle":"2025-08-15T12:43:15.661433Z","shell.execute_reply.started":"2025-08-15T12:43:15.647708Z","shell.execute_reply":"2025-08-15T12:43:15.660472Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56bfd2fc78c646e9bac177b21113b2a3"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Save the fine-tuned LoRA adapters\nprint(\"\\n--- Saving LoRA Adapters ---\")\nmodel.save_pretrained(\"gemma-3-270m-finance-lora\")\ntokenizer.save_pretrained(\"gemma-3-270m-finance-lora\")\nprint(\"Model adapters saved to 'gemma-3-270m-finance-lora'\")\n\n# Push the model adapters and tokenizer to the Hub\nrepo_name = \"huseyincavus/gemma-3-270m-finance-lora\"\n\nprint(f\"\\n--- Pushing LoRA Adapters to Hugging Face Hub ({repo_name}) ---\")\nmodel.push_to_hub(repo_name, token = True)\ntokenizer.push_to_hub(repo_name, token = True)\nprint(\"Model adapters and tokenizer pushed to Hugging Face Hub!\")\n","metadata":{"trusted":true,"id":"qoTuxoAbF1ZE","execution":{"iopub.status.busy":"2025-08-15T12:43:23.022258Z","iopub.execute_input":"2025-08-15T12:43:23.022930Z","iopub.status.idle":"2025-08-15T12:43:31.545327Z","shell.execute_reply.started":"2025-08-15T12:43:23.022907Z","shell.execute_reply":"2025-08-15T12:43:31.544679Z"}},"outputs":[{"name":"stdout","text":"\n--- Saving LoRA Adapters ---\nModel adapters saved to 'gemma-3-270m-finance-lora'\n\n--- Pushing LoRA Adapters to Hugging Face Hub (huseyincavus/gemma-3-270m-finance-lora) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"295bcc444d05489aac260cca5fbe1294"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aefbe8a77c5a434fa8d98f5f0d1997bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/15.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00abcaaf7fbc4ef9ace1dadae68850a5"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/huseyincavus/gemma-3-270m-finance-lora\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cecfce9dcbf45c18bfb29d718505ca2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8613ae808e704ca3aec9067fdf41b300"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7015c0e1d5f14137aedd5efe9eb995f0"}},"metadata":{}},{"name":"stdout","text":"Model adapters and tokenizer pushed to Hugging Face Hub!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"AFTER RESTART","metadata":{"id":"FjRBElZhF1ZE"}},{"cell_type":"code","source":"# Merge base model + adapters and push to Hub\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Define the base model and LoRA adapter\nbase_model_id = \"unsloth/gemma-3-270m-it\"\nlora_adapter_id = \"huseyincavus/gemma-3-270m-finance-lora\"\nmerged_model_id = \"huseyincavus/gemma-3-270m-finance-merged\"\n\n# Load the base model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# Load the LoRA adapter and merge it with the base model\nmodel = PeftModel.from_pretrained(model, lora_adapter_id)\nmodel = model.merge_and_unload()\n\n# Push the merged model and tokenizer to the Hugging Face Hub\nmodel.push_to_hub(merged_model_id)\ntokenizer.push_to_hub(merged_model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:46:09.266436Z","iopub.execute_input":"2025-08-15T12:46:09.267175Z","iopub.status.idle":"2025-08-15T12:46:25.510451Z","shell.execute_reply.started":"2025-08-15T12:46:09.267140Z","shell.execute_reply":"2025-08-15T12:46:25.509779Z"},"id":"dOipj4kWF1ZF"},"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/959 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f97f2d85073d44529682d5b34a1251b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/15.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4621dcccb1e49e7acd40980d16aa6ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55169dc8b58a493ba40502866c3e73a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e853f40de19d4d8580dd3943f7ba8380"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9411b09f8baa4a849c733343c4926d80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75a2e56ebd55467da73acf8163fec87d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50360c5fc9174725990506cd8893bf11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ff14d86c4bd4ec78d51d997075ff6c2"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/huseyincavus/gemma-3-270m-finance-merged/commit/c4920974123c4e1f9bc0b2b9fccde75bb717bfe0', commit_message='Upload tokenizer', commit_description='', oid='c4920974123c4e1f9bc0b2b9fccde75bb717bfe0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/huseyincavus/gemma-3-270m-finance-merged', endpoint='https://huggingface.co', repo_type='model', repo_id='huseyincavus/gemma-3-270m-finance-merged'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":19}]}